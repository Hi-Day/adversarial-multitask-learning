{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428faa9f",
   "metadata": {
    "id": "428faa9f"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6348236a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69321cb",
   "metadata": {
    "id": "c69321cb"
   },
   "outputs": [],
   "source": [
    "AMAZON = \"amazon\"\n",
    "DSLR = \"dslr\"\n",
    "WEBCAM = \"webcam\"\n",
    "\n",
    "DIR_RESULT = \"result/\"\n",
    "\n",
    "DIR_AMAZON = \"office31/{}\".format(AMAZON)\n",
    "DIR_DSLR = \"office31/{}\".format(DSLR)\n",
    "DIR_WEBCAM = \"office31/{}\".format(WEBCAM)\n",
    "\n",
    "################################## config #################################\n",
    "\n",
    "# 0, 1, 2, 3, 4\n",
    "M, N, O = 2, 0, 0\n",
    "\n",
    "DIR_SOURCE = DIR_AMAZON\n",
    "DIR_TARGET = DIR_WEBCAM\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "if DIR_SOURCE == DIR_AMAZON:\n",
    "    SOURCE = AMAZON\n",
    "elif DIR_SOURCE == DIR_DSLR:\n",
    "    SOURCE = DSLR\n",
    "elif DIR_SOURCE == DIR_WEBCAM:\n",
    "    SOURCE = WEBCAM\n",
    "    \n",
    "if DIR_TARGET == DIR_AMAZON:\n",
    "    SOURCE = AMAZON\n",
    "elif DIR_TARGET == DIR_DSLR:\n",
    "    SOURCE = DSLR\n",
    "elif DIR_TARGET == DIR_WEBCAM:\n",
    "    SOURCE = WEBCAM\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "IMG_DIM = 3\n",
    "\n",
    "LATENT_DIM = 128\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "scales = [0, 0.1, 0.3, 0.6, 0.01]\n",
    "\n",
    "W_CATEGORICAL = scales[M]\n",
    "W_ADVERSARIAL = scales[N]\n",
    "W_DOMAIN      = scales[O]\n",
    "    \n",
    "TITLE = \"{}_{} = {}-{}-{}\".format(AMAZON, WEBCAM, W_CATEGORICAL, W_ADVERSARIAL, W_DOMAIN)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e79d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amazon_webcam = 0.1-0.01-0.01'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd435acb",
   "metadata": {
    "id": "bd435acb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 07:30:38.258927: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 07:30:38.395081: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-11 07:30:39.025806: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-11 07:30:39.025853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-11 07:30:39.025859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, source, target, BATCH_SIZE=64, IMG_HEIGHT = 128, IMG_WIDTH = 128, IMG_DIM = 3):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.IMG_HEIGHT = IMG_HEIGHT\n",
    "        self.IMG_WIDTH = IMG_WIDTH\n",
    "        self.IMG_DIM = IMG_DIM\n",
    "        self.normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "    def load_source_train(self):\n",
    "        dataset_source =  tf.keras.utils.image_dataset_from_directory(\n",
    "                                self.source,\n",
    "                                validation_split=0.2,\n",
    "                                subset=\"training\",\n",
    "                                seed=123,\n",
    "                                image_size=(self.IMG_HEIGHT, self.IMG_WIDTH),\n",
    "                                batch_size=self.BATCH_SIZE)\n",
    "\n",
    "        return dataset_source.map(lambda x, y: (self.normalization_layer(x), y))\n",
    "\n",
    "\n",
    "    def load_source_validation(self):\n",
    "        dataset_source_val =  tf.keras.utils.image_dataset_from_directory(\n",
    "                                self.source,\n",
    "                                validation_split=0.2,\n",
    "                                subset=\"validation\",\n",
    "                                seed=123,\n",
    "                                image_size=(self.IMG_HEIGHT, self.IMG_WIDTH),\n",
    "                                batch_size=self.BATCH_SIZE)\n",
    "\n",
    "        return dataset_source_val.map(lambda x, y: (self.normalization_layer(x), y))\n",
    "\n",
    "    def load_target_test(self):\n",
    "        dataset_target =  tf.keras.utils.image_dataset_from_directory(\n",
    "                                self.target,\n",
    "                                seed=123,\n",
    "                                image_size=(self.IMG_HEIGHT, self.IMG_WIDTH),\n",
    "                                batch_size=self.BATCH_SIZE)\n",
    "\n",
    "        return dataset_target.map(lambda x, y: (self.normalization_layer(x), y))\n",
    "\n",
    "    def load(self):\n",
    "        return tf.data.Dataset.zip((self.load_source_train(), self.load_source_validation(), self.load_target_test()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b72c50",
   "metadata": {
    "id": "d8b72c50"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0lTMiIFmYCr_",
   "metadata": {
    "id": "0lTMiIFmYCr_"
   },
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -dy\n",
    "    return y, custom_grad\n",
    "\n",
    "class GradReverse(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26cfb3b9",
   "metadata": {
    "id": "26cfb3b9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class AML(tf.keras.Model):\n",
    "    \"\"\"Adeversarial Multitask Learning.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_adversarial=0, weight_categorical=0, weight_domain=0):\n",
    "        super(AML, self).__init__()\n",
    "\n",
    "        self.InceptionV3 = tf.keras.applications.InceptionV3(\n",
    "                            weights='imagenet',\n",
    "                            input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DIM),\n",
    "                            include_top=False)\n",
    "\n",
    "        self.InceptionV3.trainable = False\n",
    "\n",
    "        self.feature_extractor = keras.Sequential(\n",
    "                [\n",
    "                    self.InceptionV3,\n",
    "                    layers.GlobalAveragePooling2D(),\n",
    "                    layers.Flatten(),\n",
    "                    layers.Dropout(0.5),\n",
    "                    layers.Dense(512, activation='relu', kernel_initializer='he_uniform'),\n",
    "\n",
    "                ],\n",
    "                name=\"feature_extractor\",\n",
    "            )\n",
    "\n",
    "        self.categorical_classifier = keras.Sequential(\n",
    "                [\n",
    "                    layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
    "                    layers.Dropout(0.3),\n",
    "                    layers.Dense(31, activation='softmax'),\n",
    "                ],\n",
    "                name=\"categorical_classifier\",\n",
    "            )\n",
    "\n",
    "        self.domain_classifier = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
    "                layers.Dropout(0.3),\n",
    "                GradReverse(),\n",
    "                layers.Dense(1),\n",
    "            ],\n",
    "            name=\"domain_classifier\",\n",
    "        )\n",
    "\n",
    "        self.generator = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(LATENT_DIM,)),\n",
    "                layers.Dense(8 * 8 * 128),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Reshape((8, 8, 128)),\n",
    "                layers.Conv2DTranspose(128, (4, 4), strides=(4, 4), padding=\"same\"),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Conv2DTranspose(128, (4, 4), strides=(4, 4), padding=\"same\"),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Conv2D(3, (8, 8), padding=\"same\", activation=\"sigmoid\"),\n",
    "            ],\n",
    "            name=\"generator\",\n",
    "        )\n",
    "\n",
    "        self.discriminator = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(1),\n",
    "            ],\n",
    "            name=\"discriminator\",\n",
    "        )\n",
    "\n",
    "        self.weight_categorical = weight_categorical\n",
    "        self.weight_adversarial = weight_adversarial\n",
    "        self.weight_domain      = weight_domain\n",
    "\n",
    "        self.d_optimizer = keras.optimizers.Adam()\n",
    "        self.g_optimizer = keras.optimizers.Adam()\n",
    "        self.c_optimizer = keras.optimizers.Adam()\n",
    "        self.fe_optimizer = keras.optimizers.Adam()\n",
    "        self.domain_optimizer = keras.optimizers.Adam()\n",
    "\n",
    "        self.val_accuracy = tf.keras.metrics.Accuracy()\n",
    "        self.train_accuracy = tf.keras.metrics.Accuracy()\n",
    "        self.target_accuracy = tf.keras.metrics.Accuracy()\n",
    "        \n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.loss_fn_cls = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        random_latent_vectors = tf.random.normal(shape=(BATCH_SIZE, LATENT_DIM))\n",
    "        data = self.generator(random_latent_vectors)\n",
    "        features = self.feature_extractor(data)\n",
    "        cat_cls = self.categorical_classifier(features)\n",
    "        dom_cls = self.domain_classifier(features)\n",
    "        dis_cls = self.discriminator(features)\n",
    "\n",
    "        return cat_cls, dom_cls, dis_cls\n",
    "\n",
    "    def predict_domain(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.domain_classifier(features)\n",
    "\n",
    "    def predict_category(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.categorical_classifier(features)\n",
    "\n",
    "    def predict_adversarial(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.discriminator(features)\n",
    "\n",
    "    def generate_data(self):\n",
    "        random_latent_vectors = tf.random.normal(shape=(BATCH_SIZE, LATENT_DIM))\n",
    "        return self.generator(random_latent_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2BtUtzLIs2Gb",
   "metadata": {
    "id": "2BtUtzLIs2Gb"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, real_images, real_label, test_images, test_label, target_images, target_label):\n",
    "\n",
    "    #adversarial data\n",
    "    generated_images = model.generate_data()\n",
    "    combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "    combined_labels = tf.concat([tf.ones((BATCH_SIZE, 1)), tf.zeros((BATCH_SIZE, 1))], axis=0)\n",
    "#     combined_labels = tf.concat([tf.ones((BATCH_SIZE, 1)), tf.zeros((real_images.shape[0], 1))], axis=0)\n",
    "    combined_labels += 0.05 * tf.random.uniform(combined_labels.shape)\n",
    "\n",
    "    #domain\n",
    "    combined_domain = tf.concat([target_images, real_images], axis=0)\n",
    "    labels_domain = tf.concat([tf.ones((BATCH_SIZE, 1)), tf.zeros((BATCH_SIZE, 1))], axis=0)\n",
    "#     labels_domain = tf.concat([tf.ones((target_images.shape[0], 1)), tf.zeros((real_images.shape[0], 1))], axis=0)\n",
    "    labels_domain += 0.05 * tf.random.uniform(labels_domain.shape)\n",
    "\n",
    "    # Train the discriminator, cat_classifier, dom_classifier\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        predictions_disc = model.predict_adversarial(combined_images)\n",
    "        d_loss = model.loss_fn(combined_labels, predictions_disc)\n",
    "\n",
    "        predictions_clas = model.predict_category(real_images)\n",
    "        c_loss = model.loss_fn_cls(real_label, predictions_clas)\n",
    "\n",
    "        predictions_domain = model.predict_domain(combined_domain)\n",
    "        domain_loss = -1 * model.loss_fn(labels_domain, predictions_domain)\n",
    "        # domain_loss = -1 * domain_loss\n",
    "\n",
    "        fe_loss = model.weight_adversarial * d_loss + model.weight_categorical * c_loss + model.weight_domain * domain_loss\n",
    "\n",
    "    grads_feature_extractor = tape.gradient(fe_loss, model.feature_extractor.trainable_weights)\n",
    "    model.fe_optimizer.apply_gradients(zip(grads_feature_extractor, model.feature_extractor.trainable_weights))\n",
    "\n",
    "    grads_discriminator = tape.gradient(d_loss, model.discriminator.trainable_weights)\n",
    "    model.d_optimizer.apply_gradients(zip(grads_discriminator, model.discriminator.trainable_weights))\n",
    "\n",
    "    grads_categorical = tape.gradient(c_loss, model.categorical_classifier.trainable_weights)\n",
    "    model.c_optimizer.apply_gradients(zip(grads_categorical, model.categorical_classifier.trainable_weights))\n",
    "\n",
    "    grads_domain = tape.gradient(domain_loss, model.domain_classifier.trainable_weights)\n",
    "    model.domain_optimizer.apply_gradients(zip(grads_domain, model.domain_classifier.trainable_weights))\n",
    "\n",
    "    # Train generator\n",
    "    misleading_labels = tf.zeros((BATCH_SIZE, 1))\n",
    "    with tf.GradientTape() as tape:\n",
    "        generated_images = model.generate_data()\n",
    "        predictions = model.predict_adversarial(generated_images)\n",
    "        g_loss = model.loss_fn(misleading_labels, predictions)\n",
    "    grads = tape.gradient(g_loss, model.generator.trainable_weights)\n",
    "    model.g_optimizer.apply_gradients(zip(grads, model.generator.trainable_weights))\n",
    "\n",
    "    c_acc_t = model.train_accuracy(tf.math.argmax(model.predict_category(real_images), 1) , real_label)\n",
    "    c_acc_v = model.val_accuracy(tf.math.argmax(model.predict_category(test_images), 1), test_label)\n",
    "    c_acc_target = model.target_accuracy(tf.math.argmax(model.predict_category(target_images), 1), target_label)\n",
    "\n",
    "    return c_acc_target, c_acc_v, c_acc_t, domain_loss, c_loss, d_loss, g_loss, fe_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b197458d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b197458d",
    "outputId": "eb74d6e2-45dd-4014-a0c3-f421e5535e7f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2817 files belonging to 31 classes.\n",
      "Using 2254 files for training.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 07:30:40.321489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 07:30:40.923376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22822 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:37:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2817 files belonging to 31 classes.\n",
      "Using 563 files for validation.\n",
      "Found 795 files belonging to 31 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = DataLoader(DIR_SOURCE, DIR_TARGET).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5n9tSvTZs9eW",
   "metadata": {
    "id": "5n9tSvTZs9eW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# @tf.function\n",
    "def train(model, dataset, EPOCHS):\n",
    "\n",
    "    gl_, al_, cl_, dl_, tl_, acc_source_, acc_target_, acc_source_val_ = [], [], [], [], [], [], [], []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        model.train_accuracy.reset_state()\n",
    "        model.val_accuracy.reset_state()\n",
    "        model.target_accuracy.reset_state()\n",
    "        \n",
    "        for step, ((real_images, real_label), (test_images, test_label), (target_images, target_label)) in enumerate(dataset):\n",
    "            \n",
    "            \n",
    "            acc_target, acc_source_val, acc_source, domain_loss, categorical_loss, adversarial_loss, generative_loss, fe_loss = train_step(model, real_images, real_label, test_images, test_label, target_images, target_label)\n",
    "            \n",
    "            \n",
    "            gl_ += [generative_loss]\n",
    "            al_ += [adversarial_loss]\n",
    "            cl_ += [categorical_loss]\n",
    "            dl_ += [domain_loss]\n",
    "            tl_ += [fe_loss]\n",
    "            acc_source_ += [acc_source]\n",
    "            acc_target_ += [acc_target]\n",
    "            acc_source_val_ +=[acc_source_val]\n",
    "            \n",
    "        done = time.time()\n",
    "        elapsed = done - start\n",
    "        \n",
    "        if(epoch % 10 == 0):\n",
    "            print(\"Epoch {} - Time {}\".format(epoch, elapsed))\n",
    "\n",
    "    return gl_, al_, cl_, dl_, tl_, acc_source_, acc_target_, acc_source_val_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427cdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_values(gl_, al_, cl_, dl_, tl_):\n",
    "    x = np.arange(len(gl_))\n",
    "\n",
    "    plt.plot(x, gl_, label = \"generative loss\", linestyle=\"-.\")\n",
    "    plt.plot(x, al_, label = \"adversarial loss\", linestyle=\"-\")\n",
    "    plt.plot(x, cl_, label = \"categorical loss\", linestyle=\"--\")\n",
    "    plt.plot(x, dl_, label = \"domain loss\", linestyle=\":\")\n",
    "    plt.plot(x, tl_, label = \"total loss\", linestyle=(0, (3, 1, 1, 1)))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ba1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_values(acc_source_, acc_target_, acc_source_val_):\n",
    "    x = np.arange(len(acc_source_))\n",
    "    plt.plot(x, acc_source_, label = \"source acc\", linestyle=\"-\")\n",
    "    plt.plot(x, acc_target_, label = \"target acc\", linestyle=\":\")\n",
    "    plt.plot(x, acc_source_val_, label = \"val acc\", linestyle=\"-.\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf72047",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1559f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 07:30:45.529217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2024-01-11 07:30:46.183501: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-11 07:31:08.343308: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1ebab9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-11 07:31:08.343345: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\n",
      "2024-01-11 07:31:08.350847: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-11 07:31:08.429735: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-11 07:31:08.493542: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Time 37.4553427696228\n",
      "Epoch 10 - Time 2.8559610843658447\n",
      "Epoch 20 - Time 2.9386463165283203\n",
      "Epoch 30 - Time 2.9449992179870605\n",
      "Epoch 40 - Time 2.915640115737915\n",
      "Epoch 50 - Time 3.009892463684082\n",
      "Epoch 60 - Time 2.9552791118621826\n",
      "Epoch 70 - Time 2.961894989013672\n",
      "Epoch 80 - Time 2.9979655742645264\n",
      "Epoch 90 - Time 2.9235174655914307\n",
      "CPU times: user 16min 13s, sys: 2min 3s, total: 18min 16s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "FinalResult = []\n",
    "            \n",
    "model = AML(weight_adversarial=W_ADVERSARIAL, weight_categorical=W_CATEGORICAL, weight_domain=W_DOMAIN)\n",
    "model.sample()\n",
    "\n",
    "gl_, al_, cl_, dl_, tl_, acc_source_, acc_target_, acc_source_val_ = train(model, dataset, EPOCHS)\n",
    "\n",
    "# plot_loss_values(gl_, al_, cl_, dl_, tl_)\n",
    "# plot_acc_values(acc_source_, acc_target_, acc_source_val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b4905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempResult = {'W_CATEGORICAL' : W_CATEGORICAL,\n",
    "              'W_ADVERSARIAL' : W_ADVERSARIAL,\n",
    "              'W_DOMAIN'      : W_DOMAIN,\n",
    "              'gl_':gl_, \n",
    "              'al_':al_, \n",
    "              'cl_':cl_, \n",
    "              'dl_':-1 * dl_, \n",
    "              'tl_':tl_, \n",
    "              'acc_source_':acc_source_, \n",
    "              'acc_target_':acc_target_, \n",
    "              'acc_source_val_':acc_source_val_}\n",
    "\n",
    "FinalResult += [tempResult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e0c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR_RESULT + TITLE + '.pkl', 'wb') as fp:\n",
    "    pickle.dump({'result':FinalResult}, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d6f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
